{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be49a3c9",
        "outputId": "2e1b5e67-9736-4064-e2cf-4a98c6511359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import math\n",
        "import re\n",
        "from gensim import models\n",
        "from sklearn import metrics\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"universal_tagset\")\n",
        "nltk.download(\"punkt\")"
      ],
      "id": "be49a3c9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3f8d66bd"
      },
      "outputs": [],
      "source": [
        "sentences = brown.tagged_sents(tagset=\"universal\")\n",
        "word_tag=brown.tagged_words(tagset='universal')\n",
        "words_tags=np.array(word_tag)\n",
        "unique_tags=np.unique(words_tags[:,1])\n",
        "tag_dic ={ }\n",
        "rev_dic={}\n",
        "tag_dic[\"pad\"]=0\n",
        "rev_dic[0]=\"pad\"\n",
        "for A, B in zip(unique_tags, np.arange(unique_tags.size)):\n",
        "    tag_dic[A] = B+1\n",
        "    rev_dic[B+1]=A\n"
      ],
      "id": "3f8d66bd"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "832bffa1"
      },
      "outputs": [],
      "source": [
        "def remove_comma(match_obj):\n",
        "    if match_obj.group(2) is not None:\n",
        "        return match_obj.group(1)+match_obj.group(3)\n",
        "def remove_1dash(match_obj):\n",
        "    if match_obj.group(1) is not None:\n",
        "        return match_obj.group(2)\n",
        "def preprocess_word(w):\n",
        "    w=w.lower()\n",
        "    w=re.sub(\"n't\",\"\",w)\n",
        "    w=re.sub(\"ed$\",\"ing\",w)\n",
        "    w= re.sub(r\"([a-zA-Z])(,+)([a-zA-Z])\", remove_comma, w)\n",
        "    w= re.sub(r\"([0-9]+)([,.]+)([0-9]+)\", remove_comma, w)\n",
        "    w=re.sub(\"[0-9]+\",\"1\",w)\n",
        "    w=re.sub(\"\\$.+\",\"$\",w)\n",
        "    w= re.sub(r\"(1\\-)(.+)\", remove_1dash, w)\n",
        "    return w"
      ],
      "id": "832bffa1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5b1b85bb"
      },
      "outputs": [],
      "source": [
        "# sentences1 = []\n",
        "# words = []\n",
        "# for sentence in sentences:\n",
        "#     sentence1 = []\n",
        "#     for word in sentence:\n",
        "#         sentence1.append([preprocess_word(word[0]),word[1]])\n",
        "#         words.append(preprocess_word(word[0]))\n",
        "#     sentences1.append(sentence1)"
      ],
      "id": "5b1b85bb"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "abbbee58"
      },
      "outputs": [],
      "source": [
        "# words = np.unique(np.sort(np.array(words)))"
      ],
      "id": "abbbee58"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bad4d3e1"
      },
      "outputs": [],
      "source": [
        "# wordDict = {}\n",
        "# for i in range(len(words)):\n",
        "#     wordDict[words[i]] = i"
      ],
      "id": "bad4d3e1"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "92a19af6"
      },
      "outputs": [],
      "source": [
        "# wordDict"
      ],
      "id": "92a19af6"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "84f51c5c-285e-4b06-8af8-ba6e17224d46"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "# wv = api.load('word2vec-google-news-300')\n",
        "# wv.save(\"google_wv.wordvectors\")\n",
        "# vec_king = wv['king']\n",
        "wv = KeyedVectors.load(\"google_wv.wordvectors\", mmap='r')"
      ],
      "id": "84f51c5c-285e-4b06-8af8-ba6e17224d46"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c3c0247c-0409-475d-9df8-437be887cf5e"
      },
      "outputs": [],
      "source": [
        "# def getVectorFromW2V(str):\n",
        "#     if str in wv:\n",
        "#         return wv[str]\n",
        "#     else:\n",
        "#         return None\n",
        "# found=0\n",
        "# notfound=0\n",
        "# w2v_embeddings = np.random.normal(0,1,(len(wordDict) + 1, 300))\n",
        "# for word,i in wordDict.items():\n",
        "#     w2v_vector = getVectorFromW2V(word)\n",
        "#     if w2v_vector is not None:\n",
        "#         found+=1\n",
        "#         w2v_embeddings[i] = w2v_vector\n",
        "#     else:\n",
        "#       notfound+=1\n",
        "\n"
      ],
      "id": "c3c0247c-0409-475d-9df8-437be887cf5e"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5dXkq43ENWJY"
      },
      "outputs": [],
      "source": [
        "def get_vector_sentence(sentence,n,present=None):\n",
        "    length=len(sentence)\n",
        "    no_parts=math.ceil(length/n)\n",
        "    pads=length-(n*no_parts)\n",
        "    labels=np.zeros((no_parts,n))\n",
        "    word_v=np.random.normal(0,1,(no_parts,n,300))\n",
        "    for i in range(no_parts):\n",
        "      for j in range(n):\n",
        "        if n*i+j>=length:\n",
        "          word_v[i,j,:]=np.zeros(300)\n",
        "          continue\n",
        "        if present is not None:\n",
        "          word=preprocess_word(sentence[n*i+j][0])\n",
        "          labels[i,j]=tag_dic[sentence[n*i+j][1]]\n",
        "          if word in wv:\n",
        "            word_v[i,j,:]=wv[word].squeeze()\n",
        "        else:\n",
        "          word=preprocess_word(sentence[n*i+j])\n",
        "          if word in wv:\n",
        "            word_v[i,j,:]=wv[word].squeeze()\n",
        "    labels_t=np.tile(labels[:,:,None],(1,1,13))\n",
        "    labels_r=np.tile(np.arange(13)[None,None,:],(no_parts,n,1))\n",
        "    mask=labels_t==labels_r\n",
        "    labels_out=np.zeros((no_parts,n,13))\n",
        "    labels_out[mask]=1\n",
        "    return np.reshape(word_v,(no_parts,300*n)),np.reshape(labels_out,(no_parts,n*13))\n"
      ],
      "id": "5dXkq43ENWJY"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w31517ID1-Fu"
      },
      "outputs": [],
      "source": [
        "n_max=10\n",
        "# k=0\n",
        "# sent_vector_dic={}\n",
        "# sent_label_dic={}\n",
        "# for sentence in sentences:\n",
        "#     vect,labs=get_vector_sentence(sentence,n_max,1)\n",
        "#     for l in range(vect.shape[0]):\n",
        "#       sent_vector_dic[k]=vect[l]\n",
        "#       sent_label_dic[k]=labs[n_max*l:n_max*(l+1)]\n",
        "#       k+=1\n",
        "#     print(k)"
      ],
      "id": "w31517ID1-Fu"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sk9oyPRxHbR4"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "sk9oyPRxHbR4"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kNikaTD1X7zE"
      },
      "outputs": [],
      "source": [
        "def split_index(n,val_perc):         #n=size of dataset #val_perc=percentage of validation set from net train dataset.\n",
        "    n_val=int(val_perc*n)\n",
        "    index=np.random.permutation(n)\n",
        "    return index[n_val:],index[:n_val]"
      ],
      "id": "kNikaTD1X7zE"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "P2iukIBCYFaI"
      },
      "outputs": [],
      "source": [
        "batch_size=128\n",
        "total_sent=len(sentences)\n",
        "train_index,validation_index=split_index(total_sent,0.2)\n",
        "no_batches_train=math.ceil(len(train_index)/batch_size)\n",
        "no_batches_val=math.ceil(len(validation_index)/batch_size)\n",
        "current_batch_train=0\n",
        "current_batch_test=0\n",
        "train_loader=5\n",
        "val_loader=10\n"
      ],
      "id": "P2iukIBCYFaI"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yFl9hJvGA0OO"
      },
      "outputs": [],
      "source": [
        "def give_batch_train():\n",
        "  global current_batch_train\n",
        "  global current_batch_test\n",
        "  global no_batches_train\n",
        "  global no_batches_val\n",
        "  global train_loader\n",
        "  global val_loader\n",
        "  if current_batch_train==no_batches_train-1:\n",
        "    l=train_index[current_batch_train*batch_size:]\n",
        "  else:\n",
        "    l=train_index[current_batch_train*batch_size:(current_batch_train+1)*batch_size]\n",
        "  mini_size=len(l)\n",
        "  vect=None\n",
        "  labs=None\n",
        "  for i in range(mini_size):\n",
        "      vects_c,labs_c=get_vector_sentence(sentences[l[i]],n_max,1)\n",
        "      if vect is not None:\n",
        "        vect=np.concatenate((vect,vects_c),0)\n",
        "        labs=np.concatenate((labs,labs_c),0)\n",
        "      else:\n",
        "        vect=vects_c.copy()\n",
        "        labs=labs_c.copy()\n",
        "  current_batch_train=(current_batch_train+1)%no_batches_train\n",
        "  return torch.from_numpy(vect),torch.from_numpy(labs)\n",
        "\n",
        "def give_batch_test():\n",
        "  global current_batch_train\n",
        "  global current_batch_test\n",
        "  global no_batches_train\n",
        "  global no_batches_val\n",
        "  global train_loader\n",
        "  global val_loader\n",
        "  if current_batch_test==no_batches_val-1:\n",
        "    l=validation_index[current_batch_test*batch_size:]\n",
        "  else:\n",
        "    l=validation_index[current_batch_test*batch_size:(current_batch_test+1)*batch_size]\n",
        "  mini_size=len(l)\n",
        "  vect=None\n",
        "  labs=None\n",
        "  for i in range(mini_size):\n",
        "      vects_c,labs_c=get_vector_sentence(sentences[l[i]],n_max,1)\n",
        "      if vect is not None:\n",
        "        vect=np.concatenate((vect,vects_c),0)\n",
        "        labs=np.concatenate((labs,labs_c),0)\n",
        "      else:\n",
        "        vect=vects_c.copy()\n",
        "        labs=labs_c.copy()\n",
        "  current_batch_test=(current_batch_test+1)%no_batches_val\n",
        "  return torch.from_numpy(vect),torch.from_numpy(labs)"
      ],
      "id": "yFl9hJvGA0OO"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TpifHNTKYYtz"
      },
      "outputs": [],
      "source": [
        "class my_model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Linear_1=torch.nn.Linear(n_max*300,n_max*104)\n",
        "        self.Linear_2=torch.nn.Linear(n_max*104,n_max*52)\n",
        "        self.Linear_3=torch.nn.Linear(n_max*52,n_max*13)\n",
        "    def forward(self,input_batch):\n",
        "        input_batch=torch.reshape(input_batch,(-1,n_max*300))\n",
        "        output=self.Linear_1(input_batch)\n",
        "        output=torch.nn.functional.relu(output)\n",
        "        output=self.Linear_2(output)\n",
        "        output=torch.nn.functional.relu(output)\n",
        "        output=self.Linear_3(output)\n",
        "        output=torch.reshape(output,(-1,n_max,13))\n",
        "        output=torch.exp(output)\n",
        "        output2=torch.sum(output,2,True)\n",
        "        output=torch.div(output,output2)\n",
        "        output=torch.reshape(output,(-1,n_max*13))\n",
        "        return output"
      ],
      "id": "TpifHNTKYYtz"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FHGy29_2c0Du"
      },
      "outputs": [],
      "source": [
        "def loss_function(preds,labels):\n",
        "    loss=-torch.mean(torch.sum(torch.multiply(labels,torch.log(preds)),1))\n",
        "    return loss"
      ],
      "id": "FHGy29_2c0Du"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SaaAoiQOYrRr"
      },
      "outputs": [],
      "source": [
        "# input_size=300*n_max\n",
        "# num_classes=13\n",
        "model=my_model()\n",
        "# loss_function=torch.nn.functional.cross_entropy\n",
        "learning_rate=0.1\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "train_accuracy=[]\n",
        "validation_accuracy=[]"
      ],
      "id": "SaaAoiQOYrRr"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GSl8-CdgZBli"
      },
      "outputs": [],
      "source": [
        "def single_batch(model,loss_function,inputbatch,labels,optimizer=None,metric=None):\n",
        "    preds=model(inputbatch.float())\n",
        "    loss=loss_function(preds,labels)\n",
        "    if optimizer is not None:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    metric_result=None\n",
        "    if metric is not None:\n",
        "        metric_result=metric(preds,labels)\n",
        "    return loss.item(),inputbatch.shape[0],metric_result"
      ],
      "id": "GSl8-CdgZBli"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pOXce8xwZC_b"
      },
      "outputs": [],
      "source": [
        "def evaluate(model,loss_function,loader,metric=None):\n",
        "    global current_batch_train\n",
        "    global current_batch_test\n",
        "    global no_batches_train\n",
        "    global no_batches_val\n",
        "    global train_loader\n",
        "    global val_loader\n",
        "    with torch.no_grad():\n",
        "        results=[]\n",
        "        if loader==5:\n",
        "          for i in range(no_batches_train):\n",
        "            xbatch,ybatch=give_batch_train()\n",
        "            results.append(single_batch(model,loss_function,xbatch,ybatch,metric=metric))\n",
        "        else:\n",
        "          for i in range(no_batches_val):\n",
        "            xbatch,ybatch=give_batch_test()\n",
        "            results.append(single_batch(model,loss_function,xbatch,ybatch,metric=metric))\n",
        "        losses,single_batchsize,metrics=zip(*results)\n",
        "        dataset_size=np.sum(single_batchsize)\n",
        "        average_loss=n_max*np.sum(np.multiply(losses,single_batchsize))/dataset_size\n",
        "        average_metric=None\n",
        "        if metric is not None:\n",
        "            average_metric=np.sum(np.multiply(metrics,single_batchsize))/dataset_size\n",
        "        return average_loss,dataset_size,average_metric"
      ],
      "id": "pOXce8xwZC_b"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cp6oRYhPbnc_"
      },
      "outputs": [],
      "source": [
        "def accuracy(preds,labels):\n",
        "    global n_max\n",
        "    preds=torch.reshape(preds,(-1,n_max,13))\n",
        "    preds=torch.argmax(preds,dim=2)\n",
        "    labels=torch.reshape(labels,(-1,n_max,13))\n",
        "    labels=torch.argmax(labels,dim=2)\n",
        "    return torch.sum(preds==labels).item()/(preds.shape[0]*preds.shape[1])"
      ],
      "id": "cp6oRYhPbnc_"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SOggbcldbp1W"
      },
      "outputs": [],
      "source": [
        "def plot_graph(train_accuracy,validation_accuracy):\n",
        "    plt.plot(train_accuracy,color='red',label='train')\n",
        "    plt.plot(validation_accuracy,color='green',label='validation')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title(\"Accuracy vs No.of epochs\")"
      ],
      "id": "SOggbcldbp1W"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jLwUknSibsLd"
      },
      "outputs": [],
      "source": [
        "def train(epochs,model,loss_function,optimizer,metric=None):\n",
        "    global current_batch_train\n",
        "    global current_batch_test\n",
        "    global no_batches_train\n",
        "    global no_batches_val\n",
        "    global train_loader\n",
        "    global val_loader\n",
        "    for epoch in range(epochs):\n",
        "        #train the model:\n",
        "        for i in range(no_batches_train):\n",
        "          xbatch,ybatch=give_batch_train()\n",
        "          loss,_,_=single_batch(model,loss_function,xbatch,ybatch,optimizer=optimizer)\n",
        "          print(i,\" \",loss)\n",
        "        print(\"Epoch Number:\",epoch+1)\n",
        "        if metric is not None:\n",
        "            #Performance on training Set:\n",
        "            # train_loss,tr_dataset_size,tr_average_acc=evaluate(model,loss_function,train_loader,metric)\n",
        "            # print(\"Training Set Size:\",tr_dataset_size,\"  Average loss per example in training set:\",train_loss,\"  accuracy percent:\",tr_average_acc*100,\"%\")\n",
        "            #Performance on Validation Set:\n",
        "            val_loss,v_dataset_size,v_average_acc=evaluate(model,loss_function,val_loader,metric=accuracy)\n",
        "            print(\"Validation Set Size:\",v_dataset_size,\"  Average loss per example in validation set:\",val_loss,\"  accuracy percent:\",v_average_acc*100,\"%\")\n",
        "            print()\n",
        "            # train_accuracy.append(tr_average_acc)\n",
        "            validation_accuracy.append(v_average_acc)\n",
        "        else:\n",
        "            #Performance on training Set:\n",
        "            train_loss,tr_dataset_size,_=evaluate(model,loss_function,train_loader,metric)\n",
        "            print(\"Training Set Size:\",tr_dataset_size,\"  Average loss per example in training set:\",train_loss)\n",
        "            #Performance on Validation Set:\n",
        "            val_loss,v_dataset_size,_=evaluate(model,loss_function,val_loader,metric=accuracy)\n",
        "            print(\"Validation Set Size:\",v_dataset_size,\"  Average loss per example in validation set:\",val_loss)\n",
        "            print()"
      ],
      "id": "jLwUknSibsLd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "khNj4gY-_erf",
        "outputId": "b6113663-427e-491b-a597-bd3f86f0e4a8"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-245-bcb789163fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_dataset_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_average_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Set Size:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_dataset_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"  Average loss per example in validation set:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"  accuracy percent:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_average_acc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_dataset_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_average_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Set Size:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_dataset_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"  Average loss per example in Training set:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"  accuracy percent:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_average_acc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_average_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-241-eae3e21ddd92>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loss_function, loader, metric)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_batches_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mxbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgive_batch_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mybatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msingle_batchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-209-291aa3adbddf>\u001b[0m in \u001b[0;36mgive_batch_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mlabs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mvects_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabs_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_vector_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mvect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvects_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-f89c554a6808>\u001b[0m in \u001b[0;36mget_vector_sentence\u001b[0;34m(sentence, n, present)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mno_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mword_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_parts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "val_loss,v_dataset_size,v_average_acc=evaluate(model,loss_function,val_loader,metric=accuracy)\n",
        "print(\"Validation Set Size:\",v_dataset_size,\"  Average loss per example in validation set:\",val_loss,\"  accuracy percent:\",v_average_acc*100,\"%\")\n",
        "train_loss,t_dataset_size,t_average_acc=evaluate(model,loss_function,train_loader,metric=accuracy)\n",
        "print(\"Training Set Size:\",t_dataset_size,\"  Average loss per example in Training set:\",train_loss,\"  accuracy percent:\",t_average_acc*100,\"%\")\n",
        "train_accuracy.append(t_average_acc)\n",
        "validation_accuracy.append(v_average_acc)"
      ],
      "id": "khNj4gY-_erf"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vtlxucftoGop"
      },
      "outputs": [],
      "source": [
        "model3=my_model()\n",
        "model3.load_state_dict(torch.load('third.pth'))\n",
        "optimizer3=torch.optim.Adam(model3.parameters(),lr=0.00001)"
      ],
      "id": "vtlxucftoGop"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM7cWttyoZrG",
        "outputId": "fb7c6929-ca2f-4f69-aa58-54aec80d2cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0   2.9561164896889944\n",
            "1   3.0865809189725604\n",
            "2   3.207040956448348\n",
            "3   3.0326874554613115\n",
            "4   3.0681389129025094\n",
            "5   3.376988254780948\n",
            "6   3.0363884848109857\n",
            "7   3.2256866133733038\n",
            "8   3.3346203356886517\n",
            "9   3.161812474670766\n",
            "10   3.028764956068893\n",
            "11   3.207746245228713\n",
            "12   3.023549506029227\n",
            "13   3.1311860451610767\n",
            "14   3.1862557063003316\n",
            "15   3.335261300364937\n",
            "16   3.19615877561889\n",
            "17   3.1493487062739964\n",
            "18   3.115772361794189\n",
            "19   3.2855827790721333\n",
            "20   3.259774762583246\n",
            "21   3.2934399362848463\n",
            "22   3.1208256319474796\n",
            "23   3.066909872327912\n",
            "24   3.123009884149562\n",
            "25   3.2144359465809016\n",
            "26   3.2401381123924637\n",
            "27   3.166072642059624\n",
            "28   3.2211862542570637\n",
            "29   3.123276758816472\n",
            "30   3.3834242340703686\n",
            "31   3.149151699097861\n",
            "32   3.1774924702454816\n",
            "33   3.3114660093537296\n",
            "34   3.147836797709584\n",
            "35   3.022010283996375\n",
            "36   3.2706641277215414\n",
            "37   3.2365123492918944\n",
            "38   3.118031842924777\n",
            "39   3.214378226426343\n",
            "40   3.3022639543634718\n",
            "41   3.2548109658523954\n",
            "42   3.1612695014293966\n",
            "43   3.147185079597914\n",
            "44   3.0804392686954074\n",
            "45   3.1218123733441736\n",
            "46   3.1402329021714563\n",
            "47   3.0987101864411595\n",
            "48   3.1793999015926255\n",
            "49   3.2595766871987806\n",
            "50   3.1342813713285738\n",
            "51   3.1178359364124564\n",
            "52   3.2700489353422464\n",
            "53   2.9735256336322737\n",
            "54   3.3849625341002496\n",
            "55   3.1743389288916313\n",
            "56   3.2140690763215165\n",
            "57   3.1325339757185087\n",
            "58   3.055510590566954\n",
            "59   3.2005156357342517\n",
            "60   3.148568392796552\n",
            "61   3.1597494144715164\n",
            "62   3.2222131572349118\n",
            "63   3.104899981387846\n",
            "64   3.287071027638485\n",
            "65   3.18144811207997\n",
            "66   3.313140067345926\n",
            "67   3.2155663233336944\n",
            "68   3.300169847460581\n",
            "69   3.139291759481367\n",
            "70   3.1862524708775997\n",
            "71   3.0198812571802884\n",
            "72   3.255736643490678\n",
            "73   3.1720443140585064\n",
            "74   3.2489324130435517\n",
            "75   3.224884522588974\n",
            "76   3.287589078559846\n",
            "77   3.1288457410142456\n",
            "78   3.335713460538086\n",
            "79   3.247785308008925\n",
            "80   3.2102356764912363\n",
            "81   3.240877268098669\n",
            "82   3.3151436019260467\n",
            "83   3.1983615764798032\n",
            "84   3.2797628685230147\n",
            "85   3.18879658557858\n",
            "86   3.089280153100399\n",
            "87   3.231141139456953\n",
            "88   3.142524211320305\n",
            "89   3.1232243697962425\n",
            "Epoch Number: 1\n",
            "Validation Set Size: 28650   Average loss per example in validation set: 31.963979042065496   accuracy percent: 89.00104712041885 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train(1,model3,loss_function,optimizer3,accuracy)"
      ],
      "id": "mM7cWttyoZrG"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xkZhl_B7odsH"
      },
      "outputs": [],
      "source": [
        "torch.save(model3.state_dict(),'third.pth')"
      ],
      "id": "xkZhl_B7odsH"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2vStFFBFxJsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19fc4a89-ef98-4d1c-c1c8-dd0e853e440e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Size: 113784   Average loss per example in Training set: nan   accuracy percent: 89.04222034732476 %\n"
          ]
        }
      ],
      "source": [
        "train_loss,t_dataset_size,t_average_acc=evaluate(model3,loss_function,train_loader,metric=accuracy)\n",
        "print(\"Training Set Size:\",t_dataset_size,\"  Average loss per example in Training set:\",train_loss,\"  accuracy percent:\",t_average_acc*100,\"%\")"
      ],
      "id": "2vStFFBFxJsh"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkvdsLpWVbVM"
      },
      "id": "MkvdsLpWVbVM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}